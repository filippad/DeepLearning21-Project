{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MixMatch_v6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HdPUmwf4M-4"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount= True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Qo4jdzh-G6",
        "outputId": "df295024-a6e6-4e82-b574-f3409b78c9db"
      },
      "source": [
        "!pip install torch_utils"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_utils in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch_utils) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch_utils) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torch_utils) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AziIsuVg4X4W"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import gc\n",
        "import math\n",
        "from torch_utils import  AverageMeter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va2Mxic-yRlU"
      },
      "source": [
        "def displayImages(images, title1=\"Original\", title2=\"Augmented\", labels=None, augmented_images=None):\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    fig = plt.figure(figsize=(images.shape[0], images.shape[0]))\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        if labels is not None:\n",
        "            plt.xlabel(class_names[int(labels[i])])\n",
        "    fig.suptitle(title1, fontsize=16)\n",
        "\n",
        "    if augmented_images is not None:\n",
        "        fig2 = plt.figure(2, figsize=(augmented_images.shape[0], augmented_images.shape[0]))\n",
        "        for i in range(augmented_images.shape[0]):\n",
        "            plt.subplot(5, 5, i + 1)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.grid(False)\n",
        "            plt.imshow(augmented_images[i], cmap=plt.cm.binary)\n",
        "            if labels is not None:\n",
        "                plt.xlabel(class_names[int(labels[i])])\n",
        "        fig2.suptitle(title2, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def split_indexes(n_classes, n_labeled_per_class, n_validation, labels):\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_indexes = []\n",
        "    train_unlabeled_indexes = []\n",
        "    validation_indexes = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        indexes = np.where(labels == i)[0]\n",
        "        np.random.shuffle(indexes)\n",
        "\n",
        "        train_labeled_indexes.extend(indexes[:n_labeled_per_class])\n",
        "        train_unlabeled_indexes.extend(indexes[n_labeled_per_class:-n_validation])\n",
        "        validation_indexes.extend(indexes[-n_validation:])\n",
        "\n",
        "    np.random.shuffle(train_unlabeled_indexes)\n",
        "    np.random.shuffle(train_labeled_indexes)\n",
        "    np.random.shuffle(validation_indexes)\n",
        "\n",
        "    return train_labeled_indexes, train_unlabeled_indexes, validation_indexes\n",
        "\n",
        "\n",
        "def to_tensor_dim(x, source='NHWC', target='NCHW'):\n",
        "    return x.transpose([source.index(d) for d in target])\n",
        "\n",
        "\n",
        "def normalise(X):\n",
        "    mean = np.mean(X, axis=(0, 1, 2))\n",
        "    std = np.std(X, axis=(0, 1, 2))\n",
        "    X, mean, std = [np.array(a, np.float32) for a in (X, mean, std)]\n",
        "    X -= mean\n",
        "    X *= 1.0 / std\n",
        "    return X\n",
        "\n",
        "\n",
        "def random_flip(x):\n",
        "    if np.random.rand() < 0.6:\n",
        "        x = x[:, ::-1, :]\n",
        "    return x.copy()\n",
        "\n",
        "\n",
        "def pad(x, border=4):\n",
        "    return np.pad(x, [(border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "\n",
        "def pad_and_crop(x, output_size=(32, 32)):\n",
        "    x = pad(x, 4)\n",
        "    h, w = x.shape[:-1]\n",
        "    new_h, new_w = output_size\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    x = x[top: top + new_h, left: left + new_w, :]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def augment(X, K=1):\n",
        "\n",
        "    # X_augmented = X\n",
        "    # for k in range(K-1):\n",
        "    #   X_augmented.hstack(X)\n",
        "    X_augmented = np.zeros(([K]+list(X.shape)))\n",
        "\n",
        "    # print(\"after hstack\", X_augmented.shape)\n",
        "    for k in range(K):\n",
        "      for i in range(X_augmented[k].shape[0]):\n",
        "            x = X[i, :]\n",
        "            x = pad_and_crop(x)\n",
        "            X_augmented[k, i, :] = random_flip(x)\n",
        "    return X_augmented\n",
        "\n",
        "\n",
        "def load_and_augment_data(dataset_name, model_params):\n",
        "    \"\"\"\n",
        "    From datasets.CIFAR10:\n",
        "        dataset.data: the image as numpy array, shape: (50000, 32, 32, 3)\n",
        "        dataset.targets: labels of the images as list, len: 50000\n",
        "    :return:\n",
        "        augmented_labeled_X: the tensor of augmented labeled images (K=1),\n",
        "                             size: (n_labeled_per_class * n_classes , 32, 32, 3)\n",
        "        augmented_unlabeled_X: the tensor of augmented unlabeled images (K=2),\n",
        "                             size: ((N/10 - n_labeled_per_class - n_validation) * n_classes * K , 32, 32, 3)\n",
        "        train_labeled_targets: the tensor of labeled targets,\n",
        "                             size = n_labeled_per_class * n_classes\n",
        "        train_unlabeled_targets: the tensor of unlabeled targets,\n",
        "                             size = (N/10 - n_labeled_per_class - n_validation) * n_classes\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Set the model's hyperparameters\n",
        "    n_classes = model_params[\"n_classes\"]\n",
        "    n_labeled_per_class = model_params[\"n_labeled_per_class\"]\n",
        "    n_validation = model_params[\"n_validation\"]\n",
        "    K = model_params[\"K\"]\n",
        "\n",
        "    # Step 2: Load the dataset\n",
        "    if dataset_name == 'CIFAR10':\n",
        "        dataset = datasets.CIFAR10(root=\"./datasets/cifar10/train\", train=True, download=True)\n",
        "        test_set = datasets.CIFAR10(root=\"./datasets/cifar10/test\", train=False, download=True)\n",
        "    elif dataset_name == 'SLT10':\n",
        "        dataset = datasets.STL10(root=\"./datasets/slt10/train\", split='train', download=True)\n",
        "        test_set = datasets.STL10(root=\"./datasets/slt10/test\", split='test', download=True)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset name\")\n",
        "\n",
        "    # Step 3: Split the indexes\n",
        "    train_labeled_indexes, train_unlabeled_indexes, validation_indexes = \\\n",
        "        split_indexes(n_classes, n_labeled_per_class, n_validation, dataset.targets)\n",
        "\n",
        "    # Step 4: Attract the images for training, validation\n",
        "    train_labeled_images = np.take(dataset.data, train_labeled_indexes, axis=0)\n",
        "    train_unlabeled_images = np.take(dataset.data, train_unlabeled_indexes, axis=0)\n",
        "    target_array = np.asarray(dataset.targets)\n",
        "    train_labeled_targets = np.take(target_array, train_labeled_indexes, axis=0)\n",
        "    train_unlabeled_targets = np.take(target_array, train_unlabeled_indexes, axis=0)\n",
        "    validation_images = np.take(dataset.data, validation_indexes, axis=0)\n",
        "    validation_targets = np.take(target_array, validation_indexes, axis=0)\n",
        "\n",
        "    # Step 5: Normalise the datasets and make the labels one-hot encoded\n",
        "    train_labeled_images = normalise(train_labeled_images)\n",
        "    train_unlabeled_images = normalise(train_unlabeled_images)\n",
        "\n",
        "    test_X = normalise(test_set.data)\n",
        "    test_X = to_tensor_dim(test_X)\n",
        "    test_X = torch.from_numpy(test_X)\n",
        "\n",
        "    train_labeled_targets = torch.from_numpy(train_labeled_targets)\n",
        "    train_unlabeled_targets = torch.from_numpy(train_unlabeled_targets)\n",
        "    validation_targets = torch.from_numpy(validation_targets)\n",
        "    test_targets = torch.from_numpy(np.asarray(test_set.targets))\n",
        "\n",
        "    # Step 6: Augment training images\n",
        "    print(\"shape\",train_unlabeled_images.shape )\n",
        "\n",
        "    augmented_labeled_X = augment(train_labeled_images, K=1)\n",
        "    augmented_unlabeled_X = augment(train_unlabeled_images, K=K)\n",
        "    \n",
        "    print(\"shape after\", augmented_unlabeled_X.shape )\n",
        "    \n",
        "    # Take a look at some of the augmented images\n",
        "    # displayImages(train_labeled_images[:10], title1=\"Original-Labeled\", title2=\"Augmented-Labeled\",\n",
        "    #               augmented_images=augmented_labeled_X[:10], labels=train_labeled_targets[:10])\n",
        "    # n_unlabeled = train_unlabeled_images.shape[0]\n",
        "    # displayImages(train_unlabeled_images[:10], title1=\"Original-Unlabeled\", title2=\"Augmented-Unlabeled\",\n",
        "    #               augmented_images=augmented_unlabeled_X[:10], labels=train_unlabeled_targets[:10])\n",
        "    # displayImages(augmented_unlabeled_X[:10], title1=\"Augmented-Unlabeled1\", title2=\"Augmented-Unlabeled2\",\n",
        "    #               augmented_images=augmented_unlabeled_X[n_unlabeled:10+n_unlabeled],\n",
        "    #               labels=train_unlabeled_targets[:10])\n",
        "\n",
        "    # Step 7: Change the dimension of np.array in oder for it to work in torch\n",
        "    augmented_labeled_X = to_tensor_dim(augmented_labeled_X.reshape(train_labeled_images.shape))\n",
        "\n",
        "    augmented_unlabeled_X_zeros = np.zeros(([K]+[augmented_unlabeled_X.shape[1]] + list(augmented_labeled_X.shape[1:])))\n",
        "    #print(\"augmented_unlabeled_X_zeros\", augmented_unlabeled_X_zeros.shape)\n",
        "    for k in range(K):\n",
        "      unlabeled_shape = augmented_unlabeled_X[k].reshape(train_unlabeled_images.shape)\n",
        "      #print(\"unlabeled_shape\", unlabeled_shape)\n",
        "      augmented_unlabeled_X_zeros[k] = to_tensor_dim(unlabeled_shape)\n",
        "    validation_images = to_tensor_dim(validation_images)\n",
        "    \n",
        "    return torch.from_numpy(augmented_labeled_X), torch.from_numpy(augmented_unlabeled_X_zeros), \\\n",
        "           train_labeled_targets, train_labeled_targets, \\\n",
        "           torch.from_numpy(validation_images), validation_targets, test_X, test_targets\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVktFDszObih"
      },
      "source": [
        "# Defining Model Parameters and obtaining train and val sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t_9ML18yasD",
        "outputId": "ac93e528-74c6-449d-ba1a-ea3b8526903f"
      },
      "source": [
        "model_params = {\n",
        "    \"n_classes\": 10,\n",
        "    \"n_labeled_per_class\": 30,\n",
        "    \"n_validation\": 500,\n",
        "    \"K\": 3\n",
        "}\n",
        "augmented_labeled_X, augmented_unlabeled_X, train_labeled_targets, train_unlabeled_targets, \\\n",
        "    validation_images, validation_targets, test_images, test_targets = load_and_augment_data('CIFAR10', model_params)\n",
        "\n",
        "print(f\"augmented_labeled_X: {augmented_labeled_X.size()}\")\n",
        "print(f\"augmented_unlabeled_X: {augmented_unlabeled_X.size()}\")\n",
        "print(f\"train_labeled_targets: {train_labeled_targets.size()}\")\n",
        "print(f\"train_unlabeled_targets: {train_unlabeled_targets.size()}\")\n",
        "print(f\"validation_images: {validation_images.size()}\")\n",
        "print(f\"validation_targets: {validation_targets.size()}\")\n",
        "print(f\"test_targets: {test_targets.size()}\")\n",
        "print(f\"test_X: {test_images.size()}\")\n",
        "\n",
        "# Halve the number of labeled data\n",
        "# n_labeled = augmented_labeled_X.size()[0]\n",
        "# new_n = int(n_labeled/2)\n",
        "# augmented_labeled_X = augmented_labeled_X[:new_n]\n",
        "# print(f\"New length: {augmented_labeled_X.size()}\")\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "shape (44700, 32, 32, 3)\n",
            "shape after (3, 44700, 32, 32, 3)\n",
            "augmented_labeled_X: torch.Size([300, 3, 32, 32])\n",
            "augmented_unlabeled_X: torch.Size([3, 44700, 3, 32, 32])\n",
            "train_labeled_targets: torch.Size([300])\n",
            "train_unlabeled_targets: torch.Size([300])\n",
            "validation_images: torch.Size([5000, 3, 32, 32])\n",
            "validation_targets: torch.Size([5000])\n",
            "test_targets: torch.Size([10000])\n",
            "test_X: torch.Size([10000, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odGZGXOtauO8"
      },
      "source": [
        "Wide ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh8DdvN1a0m5"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0, activate_before_residual=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut and self.activate_before_residual == True:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0, activate_before_residual=False):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate, activate_before_residual):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate, activate_before_residual))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, num_classes, depth=28, widen_factor=2, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate, activate_before_residual=True)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3], momentum=0.001)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvqUDZSXIhd8"
      },
      "source": [
        "class WeightEMA(object):\n",
        "    def __init__(self, model, ema_model,lr, alpha=0.999):\n",
        "        self.model = model\n",
        "        self.ema_model = ema_model\n",
        "        self.alpha = alpha\n",
        "        self.params = list(model.state_dict().values())\n",
        "        self.ema_params = list(ema_model.state_dict().values())\n",
        "        self.wd = 0.02 * lr\n",
        "\n",
        "        for param, ema_param in zip(self.params, self.ema_params):\n",
        "            param.data.copy_(ema_param.data)\n",
        "\n",
        "    def step(self):\n",
        "        one_minus_alpha = 1.0 - self.alpha\n",
        "        for param, ema_param in zip(self.params, self.ema_params):\n",
        "            if ema_param.dtype==torch.float32:\n",
        "                ema_param.mul_(self.alpha)\n",
        "                ema_param.add_(param * one_minus_alpha)\n",
        "                # customized weight decay\n",
        "                param.mul_(1 - self.wd)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dRj0sD2UDEE"
      },
      "source": [
        "batch_size_val=10\n",
        "validation_images=list(torch.split(validation_images, batch_size_val))\n",
        "validation_targets=list(torch.split(validation_targets, batch_size_val))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ADSNERsGxtw"
      },
      "source": [
        "batch_size_test=10\n",
        "test_images=list(torch.split(test_images, batch_size_val))\n",
        "test_targets=list(torch.split(test_targets, batch_size_val))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTgTai_p30CI"
      },
      "source": [
        "#splitting into required number of batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbp2lWcuJYmV"
      },
      "source": [
        "# Splitting data into Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pok_bZgUJxco"
      },
      "source": [
        "# batch_size_labeled = int(augmented_labeled_X.size()[0]/num_batches)\n",
        "# batch_size_unlabeled = int(augmented_unlabeled_X.size()[1]/num_batches)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lug8HuaC3wQ6"
      },
      "source": [
        "batch_size_labeled = 30\n",
        "batch_size_unlabeled = 30"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z75pYfaG40B3",
        "outputId": "a5011eb1-058e-4298-d32b-bcce6407f4bf"
      },
      "source": [
        "batch_size_labeled"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6c1XbUJkA2"
      },
      "source": [
        "## Splitting labelled data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttYoAOvqk1ot"
      },
      "source": [
        "permutation = torch.randperm(augmented_labeled_X.size()[0])\n",
        "labeled_batches = []\n",
        "for i in range(0,augmented_labeled_X.size()[0], batch_size_labeled):\n",
        "        indices = permutation[i:i+batch_size_labeled]\n",
        "        batch_x, batch_y = augmented_labeled_X[indices], train_labeled_targets[indices]\n",
        "        labeled_batches.append((batch_x, batch_y))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twdB1Pm40Inp",
        "outputId": "e731f29d-1923-4cd5-e55d-fc6421a2eebd"
      },
      "source": [
        "del augmented_labeled_X,train_labeled_targets\n",
        "gc.collect()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "481"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn5yUae9Jua8"
      },
      "source": [
        "# Splitting unlabelled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMO1gm-wItV9"
      },
      "source": [
        "#Splitting unlabelled into batches\n",
        "k_unlabeled_batches = []\n",
        "for k in range(augmented_unlabeled_X.size()[0]):\n",
        "  permutation = torch.randperm(augmented_unlabeled_X.size()[1])\n",
        "  unlabeled_batches = []\n",
        "  for i in range(0, augmented_unlabeled_X.size()[1], batch_size_unlabeled):\n",
        "          indices = permutation[i:i+batch_size_unlabeled]\n",
        "          batch = augmented_unlabeled_X[k][indices]\n",
        "          unlabeled_batches.append(batch)\n",
        "  k_unlabeled_batches.append(unlabeled_batches)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcVioyFb0l38",
        "outputId": "c43fd7f4-f5ff-4f71-ec0d-ebe94bdb7b52"
      },
      "source": [
        "del augmented_unlabeled_X,unlabeled_batches\n",
        "gc.collect()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5_MqEolyPr",
        "outputId": "ca1cedd5-422a-4b00-bcdf-222426682a82"
      },
      "source": [
        "k_unlabeled_batches[0][0].size()\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vhi_NtWR3UQ"
      },
      "source": [
        "K_unlabeled_batches is a tuple of shape [k, num_batches, batch_size, 3, 32, 32]\n",
        "where k is the number of augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okMxCrpwOTfa"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHjp1npZFjyz"
      },
      "source": [
        "\n",
        "def label_guessing(model, data):\n",
        "   return model(data)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoVTPmq4SVon"
      },
      "source": [
        "# Example use of label_guessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDaNW_EYQiEe"
      },
      "source": [
        "# guesses = label_guessing(model_ft, k_unlabeled_batches[0][0].float().to(device))\n",
        "# print(f\"shape: {guesses.shape}\")\n",
        "# print(guesses)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7PlDRsg-AWl"
      },
      "source": [
        "\n",
        "def guess_and_sharpen(augmented_unlabeled_X, model, T=0.5):\n",
        "    \"\"\"\n",
        "    Assume:\n",
        "      label_guesses: a list of K guesses\n",
        "    \"\"\"\n",
        "    num_classes = 10\n",
        "    K = len(augmented_unlabeled_X)\n",
        "    softmax_guess_array = []\n",
        "    for i in range(K):\n",
        "      guess = label_guessing(model, augmented_unlabeled_X[i].float().to(device))\n",
        "      softmax_guess = torch.softmax(guess, dim=1)\n",
        "      softmax_guess_array.append(softmax_guess)\n",
        "    \n",
        "    p = softmax_guess_array[0]\n",
        "    for i in range(1,K):\n",
        "      p += softmax_guess_array[i]\n",
        "\n",
        "\n",
        "    # Sharpen\n",
        "    pt = p**(1/T)\n",
        "    targets_u = pt/pt.sum(dim=1, keepdim=True)\n",
        "    targets_u = targets_u.detach()\n",
        "\n",
        "    return targets_u\n",
        "\n",
        "\n",
        "def mixup(X_hat, data_U, targets_U, alpha):\n",
        "    data_X = X_hat[0].float().to(device)\n",
        "    targets_X = X_hat[1]\n",
        "\n",
        "    targets_X = torch.nn.functional.one_hot(targets_X, num_classes=10).float().to(device)\n",
        "\n",
        "    batch_size = data_X.size(0)\n",
        "\n",
        "    # Form W\n",
        "    all_data = data_X\n",
        "    all_targets = targets_X\n",
        "    for i in range(len(data_U)):\n",
        "      all_data = torch.cat([all_data, data_U[i].float().to(device)], dim=0)\n",
        "      all_targets = torch.cat([all_targets, targets_U], dim=0)\n",
        "    # all_data = torch.cat([data_X, data_U1.float().to(device), data_U2.float().to(device)], dim=0)\n",
        "    # all_targets = torch.cat([targets_X, targets_U, targets_U], dim=0)\n",
        "\n",
        "    idx = torch.randperm(all_data.size(0))\n",
        "    W_data = all_data[idx]\n",
        "    W_targets = all_targets[idx]\n",
        "\n",
        "    # Mix it up\n",
        "    lamda = np.random.beta(alpha, alpha)\n",
        "    lamda = max(lamda, 1 - lamda)\n",
        "\n",
        "    data_prime = lamda * all_data + (1-lamda) * W_data\n",
        "    targets_prime = lamda * all_targets + (1-lamda) * W_targets\n",
        "\n",
        "    return data_prime, targets_prime\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focbYjX2rtdK"
      },
      "source": [
        "def linear_rampup(current, rampup_length=0):\n",
        "    if rampup_length == 0:\n",
        "        return 1.0\n",
        "    else:\n",
        "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
        "        return float(current)\n",
        "\n",
        "class SemiLoss(object):\n",
        "  def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch,num_epochs):\n",
        "      probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "      Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
        "      Lu = torch.mean((probs_u - targets_u)**2)\n",
        "      lambda_u=75\n",
        "      return Lx, Lu, lambda_u * linear_rampup(epoch,num_epochs)\n",
        "# def SemiLoss(outputs_x, targets_x, outputs_u, targets_u, epoch,num_epoch):\n",
        "#   probs_u = torch.softmax(outputs_u, dim=1)\n",
        "\n",
        "#   Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
        "#   Lu = torch.mean((probs_u - targets_u)**2)\n",
        "#   lambda_u=75\n",
        "#   return Lx, Lu,lambda_u * linear_rampup(epoch,num_epoch)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BmY3WJ8yckc"
      },
      "source": [
        "class iter_count:\n",
        "  def __init__(self,max_len):\n",
        "    self.max_len=max_len\n",
        "    self.idx=None\n",
        "  def next(self):\n",
        "    if self.idx== None:\n",
        "      self.idx=0\n",
        "      return self.idx\n",
        "    elif self.idx == (self.max_len-1):\n",
        "      self.idx=0\n",
        "      return self.idx\n",
        "    else:\n",
        "      self.idx+=1\n",
        "      return self.idx"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfyVIgNoK1Gy"
      },
      "source": [
        "def interleave_offsets(batch, nu):\n",
        "    groups = [batch // (nu + 1)] * (nu + 1)\n",
        "    for x in range(batch - sum(groups)):\n",
        "        groups[-x - 1] += 1\n",
        "    offsets = [0]\n",
        "    for g in groups:\n",
        "        offsets.append(offsets[-1] + g)\n",
        "    assert offsets[-1] == batch\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def interleave(xy, batch):\n",
        "    nu = len(xy) - 1\n",
        "    offsets = interleave_offsets(batch, nu)\n",
        "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
        "    for i in range(1, nu + 1):\n",
        "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
        "    return [torch.cat(v, dim=0) for v in xy]\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63xD9oiGctPI"
      },
      "source": [
        "def train(labeled_batches, k_unlabeled_batches,epoch,n_train_iter,model, optimizer, ema_optimizer,ema_model,num_epochs):\n",
        "    # Step 1: Input of the function (Raghav)\n",
        "      # labeled_batches: List, size: n_batches, each elem: tuple of X(tensor) and targets(tensor)\n",
        "      # k_unlabeled_batches: [k, num_batches, batch_size, 3, 32, 32] where k is the number of augmentations.\n",
        "    # n_batches = len(labeled_batches)\n",
        "    loss = AverageMeter(name='loss_meter',length=n_train_iter)\n",
        "    loss_x = AverageMeter(name='loss_meter_x',length=n_train_iter)\n",
        "    loss_u = AverageMeter(name='loss_meter_u',length=n_train_iter)\n",
        "    wl = AverageMeter(name='w',length=n_train_iter)\n",
        "    acc = AverageMeter(name='acc',length=n_train_iter)\n",
        "    alpha = 0.75\n",
        "    labeled_iter=iter_count(len(labeled_batches))\n",
        "    unlabeled_iter=iter_count(len(k_unlabeled_batches[0]))\n",
        "    lc=SemiLoss()\n",
        "    # Step 2: For loop\n",
        "    for i in range(n_train_iter):\n",
        "        labeled_idx=labeled_iter.next()\n",
        "        unlabeled_idx=unlabeled_iter.next()\n",
        "        # print(labeled_idx,unlabeled_idx)\n",
        "        augmented_unlabeled_X = []\n",
        "        for k in range(len(k_unlabeled_batches)):\n",
        "          augmented_unlabeled_X.append(k_unlabeled_batches[k][unlabeled_idx])\n",
        "\n",
        "        targets_u = guess_and_sharpen(augmented_unlabeled_X, model)\n",
        "        data_prime, targets_prime = mixup(labeled_batches[labeled_idx], augmented_unlabeled_X, targets_u, alpha) \n",
        "        batch_size=labeled_batches[labeled_idx][0].size(0)\n",
        "        # print(f\"data_prime size: {data_prime.size()}\")\n",
        "        # print(f\"targets_prime size: {targets_prime.size()}\")\n",
        "        data_prime = list(torch.split(data_prime, batch_size))\n",
        "        data_prime = interleave(data_prime, batch_size)\n",
        "        log_probs = [model(data_prime[0])]\n",
        "        for input in data_prime[1:]:\n",
        "            log_probs.append(model(input))\n",
        "\n",
        "        # put interleaved samples back\n",
        "        #logits = interleave(logits, batch_size)\n",
        "        log_probs = interleave(log_probs, batch_size)\n",
        "        log_prob_x = log_probs[0]\n",
        "        log_prob_u = torch.cat(log_probs[1:], dim=0)\n",
        "\n",
        "        Lx, Lu, w = lc(log_prob_x, targets_prime[:batch_size], log_prob_u, targets_prime[batch_size:], epoch+i/n_train_iter,num_epochs)\n",
        "\n",
        "        L = Lx + w * Lu\n",
        "        loss.update(val=L.item())\n",
        "        loss_x.update(val=Lx.item())\n",
        "        loss_u.update(val=Lu.item())\n",
        "        wl.update(val=w)\n",
        "        #print(type(L))\n",
        "        optimizer.zero_grad()\n",
        "        L.backward()\n",
        "        optimizer.step()\n",
        "        ema_optimizer.step()\n",
        "        inputs1, targets1 = labeled_batches[labeled_idx][0].float().to(device), labeled_batches[labeled_idx][1].to(device,non_blocking=True)\n",
        "        outputs1=ema_model(inputs1)\n",
        "        prec1, prec5 = accuracy(outputs1, targets1, topk=(1, 5))\n",
        "        acc.update(val=prec1)\n",
        "    return loss.avg, loss_x.avg, loss_u.avg, acc.avg\n",
        "    \n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTIi0Tz2GHbV"
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qmf22EzQ43u"
      },
      "source": [
        "def validate(validation_images, validation_targets, model, use_cuda):\n",
        "    val_loss=nn.CrossEntropyLoss()\n",
        "    num_batches=len(validation_images)\n",
        "    loss = AverageMeter(name=\"loss\",length=num_batches)\n",
        "    top1 = AverageMeter(name=\"top1\",length=num_batches)\n",
        "    top5 = AverageMeter(name=\"top5\",length=num_batches)\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx in range(num_batches):\n",
        "          inputs=validation_images[batch_idx]\n",
        "          targets=validation_targets[batch_idx]\n",
        "          # targets = torch.nn.functional.one_hot(targets, num_classes=10)\n",
        "          if use_cuda:\n",
        "              inputs, targets = inputs.float().to(device), targets.to(device,non_blocking=True)\n",
        "          # compute output\n",
        "          outputs = model(inputs)\n",
        "          #print(outputs)\n",
        "          loss_eval = val_loss(outputs, targets)\n",
        "          # measure accuracy and record loss\n",
        "          prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "          loss.update(val=loss_eval.item())\n",
        "          top1.update(val=prec1.item())\n",
        "          top5.update(val=prec5.item())\n",
        "\n",
        "            \n",
        "    return (loss.avg, top1.avg)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ga2BQ2JPkvF",
        "outputId": "7c407424-223d-4233-fc10-d2b0dfc07695"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3843"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMfqXiwEbVOl"
      },
      "source": [
        "def create_model(device,ema=False):\n",
        "  model = WideResNet(num_classes=10)\n",
        "  model = model.to(device)\n",
        "\n",
        "  if ema:\n",
        "      for param in model.parameters():\n",
        "          param.detach_()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPFFbYBeiZCC"
      },
      "source": [
        "def save_model(state,checkpoint, filename='checkpoint.pth.tar'):\n",
        "    filepath = os.path.join(checkpoint, filename)\n",
        "    torch.save(state, filepath)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ckYCepgCUh",
        "outputId": "a9d3f83f-4499-4ca9-f135-b2be6411641c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_18o9iHbkpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ecd4fc-22e7-4e6f-aa11-fb6c88c27a53"
      },
      "source": [
        "lr=0.002\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model(device)\n",
        "ema_model = create_model(device,ema=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "ema_optimizer= WeightEMA(model, ema_model,lr, alpha=.999)\n",
        "resume=False\n",
        "resume_dir=\"/content/gdrive/MyDrive/DLDS_project/model/checkpoint.pth.tar\"\n",
        "save_dir=\"path to the directory to save model\"\n",
        "num_epochs=1\n",
        "n_train_iter=1500\n",
        "if resume:\n",
        "  print('==> Resuming from checkpoint..')\n",
        "  #assert os.path.isfile(args.resume), 'Error: no checkpoint directory found!'\n",
        "  # args.out = os.path.dirname(args.resume)\n",
        "  checkpoint = torch.load(resume_dir)\n",
        "  best_acc = checkpoint['best_acc']\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  ema_model.load_state_dict(checkpoint['ema_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "else:\n",
        "  start_epoch=0\n",
        "  best_acc = 0.0\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch,num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train',\"val\"]:\n",
        "        if phase == 'train':\n",
        "            train_loss, train_loss_x, train_loss_u,train_acc=train(labeled_batches, k_unlabeled_batches, epoch, n_train_iter, model, optimizer, ema_optimizer,ema_model,num_epochs)  # Set model to training mode\n",
        "            epoch_loss=train_loss\n",
        "            epoch_acc=train_acc\n",
        "\n",
        "        else:\n",
        "            loss, top1=validate(validation_images, validation_targets, ema_model, True)\n",
        "            epoch_loss=loss\n",
        "            epoch_acc=top1\n",
        "\n",
        "        \n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # deep copy the model\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            best_ema_model_wts=copy.deepcopy(ema_model.state_dict())\n",
        "            best_optimizer_wts=copy.deepcopy(optimizer.state_dict())\n",
        "\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Acc: {:4f}'.format(best_acc))\n",
        "best_state_dict={'epoch': epoch + 1,\n",
        "                'state_dict': best_model_wts,\n",
        "                'ema_state_dict':best_ema_model_wts,\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer' : best_optimizer_wts}\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "ema_model.load_state_dict(best_ema_model_wts)\n",
        "test_loss, test_acc=validate(test_images, test_targets, ema_model, True)\n",
        "print(\"Test Loss: {:4f}\".format(test_loss))\n",
        "print(\"Test Accuracy: {:4f}\".format(test_acc))\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "----------\n",
            "train Loss: 0.9612 Acc: 80.1334\n",
            "val Loss: 39.4411 Acc: 10.4200\n",
            "Training complete in 3m 25s\n",
            "Best val Acc: 10.420000\n",
            "Test Loss: 3.319487\n",
            "Test Accuracy: 9.990000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmPgP3l4-5Jm",
        "outputId": "5af2fd50-4070-4337-a94b-0f204eec917b"
      },
      "source": [
        "len(test_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuW1xGqvotKH",
        "outputId": "fde46ba7-d7b3-4ddb-84c0-4212d53663bc"
      },
      "source": [
        "best_state_dict[\"epoch\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdpolN1fiX7D"
      },
      "source": [
        "save_model(best_state_dict,checkpoint=\"/content/gdrive/MyDrive/DLDS_project/model/\", filename='checkpoint.pth.tar')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}