{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MixMatch_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f40670dac4654f839173b68e5e1f195b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f287381ea5b842c1b44f01f4c5c570d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c11d853c1a94d80ba3ed156a61c87a8",
              "IPY_MODEL_c7624da01ae2498ea56ac0b65538ca9c"
            ]
          }
        },
        "f287381ea5b842c1b44f01f4c5c570d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c11d853c1a94d80ba3ed156a61c87a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_42076a36481b4fe0a43f680973e9d781",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_504977c5b3c14b53befee17d71632dac"
          }
        },
        "c7624da01ae2498ea56ac0b65538ca9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_54d4be4b082e465ea2976f721ee68399",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:52&lt;00:00, 3233801.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9a71610cab642b39167092c366dbb15"
          }
        },
        "42076a36481b4fe0a43f680973e9d781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "504977c5b3c14b53befee17d71632dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54d4be4b082e465ea2976f721ee68399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9a71610cab642b39167092c366dbb15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HdPUmwf4M-4",
        "outputId": "baa6aba0-9f55-441a-9e8a-7ace3d5d6c0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AziIsuVg4X4W"
      },
      "source": [
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va2Mxic-yRlU"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def displayImages(images, title1=\"Original\", title2=\"Augmented\", labels=None, augmented_images=None):\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    fig = plt.figure(figsize=(images.shape[0], images.shape[0]))\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        if labels is not None:\n",
        "            plt.xlabel(class_names[int(labels[i])])\n",
        "    fig.suptitle(title1, fontsize=16)\n",
        "\n",
        "    if augmented_images is not None:\n",
        "        fig2 = plt.figure(2, figsize=(augmented_images.shape[0], augmented_images.shape[0]))\n",
        "        for i in range(augmented_images.shape[0]):\n",
        "            plt.subplot(5, 5, i + 1)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.grid(False)\n",
        "            plt.imshow(augmented_images[i], cmap=plt.cm.binary)\n",
        "            if labels is not None:\n",
        "                plt.xlabel(class_names[int(labels[i])])\n",
        "        fig2.suptitle(title2, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def split_indexes(n_classes, n_labeled_per_class, n_validation, labels):\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_indexes = []\n",
        "    train_unlabeled_indexes = []\n",
        "    validation_indexes = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        indexes = np.where(labels == i)[0]\n",
        "        np.random.shuffle(indexes)\n",
        "\n",
        "        train_labeled_indexes.extend(indexes[:n_labeled_per_class])\n",
        "        train_unlabeled_indexes.extend(indexes[n_labeled_per_class:-n_validation])\n",
        "        validation_indexes.extend(indexes[-n_validation:])\n",
        "\n",
        "    np.random.shuffle(train_unlabeled_indexes)\n",
        "    np.random.shuffle(train_labeled_indexes)\n",
        "    np.random.shuffle(validation_indexes)\n",
        "\n",
        "    return train_labeled_indexes, train_unlabeled_indexes, validation_indexes\n",
        "\n",
        "\n",
        "def to_tensor_dim(x, source='NHWC', target='NCHW'):\n",
        "    return x.transpose([source.index(d) for d in target])\n",
        "\n",
        "\n",
        "def normalise(X):\n",
        "    mean = np.mean(X, axis=(0, 1, 2))\n",
        "    std = np.std(X, axis=(0, 1, 2))\n",
        "    X, mean, std = [np.array(a, np.float32) for a in (X, mean, std)]\n",
        "    X -= mean\n",
        "    X *= 1.0 / std\n",
        "    return X\n",
        "\n",
        "\n",
        "def normalise2(X):\n",
        "    x = X.copy()\n",
        "    mean = np.mean(x, axis=(0, 1, 2)) / 255\n",
        "    std = np.std(x, axis=(0, 1, 2)) / 255\n",
        "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
        "    x -= mean * 255\n",
        "    x *= 1.0 / (255 * std)\n",
        "    return x\n",
        "\n",
        "\n",
        "def random_flip(x):\n",
        "    if np.random.rand() < 0.6:\n",
        "        x = x[:, ::-1, :]\n",
        "    return x.copy()\n",
        "\n",
        "\n",
        "def pad(x, border=4):\n",
        "    return np.pad(x, [(border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "\n",
        "def pad_and_crop(x, output_size=(32, 32)):\n",
        "    x = pad(x, 4)\n",
        "    h, w = x.shape[:-1]\n",
        "    new_h, new_w = output_size\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    x = x[top: top + new_h, left: left + new_w, :]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def augment(X, K=1):\n",
        "\n",
        "    # X_augmented = X\n",
        "    # for k in range(K-1):\n",
        "    #   X_augmented.hstack(X)\n",
        "    X_augmented = np.zeros(([K]+list(X.shape)))\n",
        "\n",
        "    # print(\"after hstack\", X_augmented.shape)\n",
        "    for k in range(K):\n",
        "      for i in range(X_augmented[k].shape[0]):\n",
        "            x = X[i, :]\n",
        "            x = pad_and_crop(x)\n",
        "            X_augmented[k, i, :] = random_flip(x)\n",
        "    return X_augmented\n",
        "\n",
        "\n",
        "def load_and_augment_data(dataset_name, model_params):\n",
        "    \"\"\"\n",
        "    From datasets.CIFAR10:\n",
        "        dataset.data: the image as numpy array, shape: (50000, 32, 32, 3)\n",
        "        dataset.targets: labels of the images as list, len: 50000\n",
        "    :return:\n",
        "        augmented_labeled_X: the tensor of augmented labeled images (K=1),\n",
        "                             size: (n_labeled_per_class * n_classes , 32, 32, 3)\n",
        "        augmented_unlabeled_X: the tensor of augmented unlabeled images (K=2),\n",
        "                             size: ((N/10 - n_labeled_per_class - n_validation) * n_classes * K , 32, 32, 3)\n",
        "        train_labeled_targets: the tensor of labeled targets,\n",
        "                             size = n_labeled_per_class * n_classes\n",
        "        train_unlabeled_targets: the tensor of unlabeled targets,\n",
        "                             size = (N/10 - n_labeled_per_class - n_validation) * n_classes\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Set the model's hyperparameters\n",
        "    n_classes = model_params[\"n_classes\"]\n",
        "    n_labeled_per_class = model_params[\"n_labeled_per_class\"]\n",
        "    n_validation = model_params[\"n_validation\"]\n",
        "    K = model_params[\"K\"]\n",
        "\n",
        "    # Step 2: Load the dataset\n",
        "    if dataset_name == 'CIFAR10':\n",
        "        dataset = datasets.CIFAR10(root=\"./datasets\", train=True, download=True)\n",
        "    elif dataset_name == 'SLT10':\n",
        "        dataset = datasets.STL10(root=\"./datasets\", download=True)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset name\")\n",
        "\n",
        "    # Step 3: Split the indexes\n",
        "    train_labeled_indexes, train_unlabeled_indexes, validation_indexes = \\\n",
        "        split_indexes(n_classes, n_labeled_per_class, n_validation, dataset.targets)\n",
        "\n",
        "    # Step 4: Attract the images for training, validation\n",
        "    train_labeled_images = np.take(dataset.data, train_labeled_indexes, axis=0)\n",
        "    train_unlabeled_images = np.take(dataset.data, train_unlabeled_indexes, axis=0)\n",
        "    target_array = np.asarray(dataset.targets)\n",
        "    train_labeled_targets = np.take(target_array, train_labeled_indexes, axis=0)\n",
        "    train_unlabeled_targets = np.take(target_array, train_unlabeled_indexes, axis=0)\n",
        "    validation_images = np.take(dataset.data, validation_indexes, axis=0)\n",
        "    validation_targets = np.take(target_array, validation_indexes, axis=0)\n",
        "\n",
        "    # Step 5: Normalise the datasets\n",
        "    train_labeled_images = normalise(train_labeled_images)\n",
        "    train_unlabeled_images = normalise(train_unlabeled_images)\n",
        "\n",
        "    # Step 6: Augment training images\n",
        "    print(\"shape\",train_unlabeled_images.shape )\n",
        "\n",
        "    augmented_labeled_X = augment(train_labeled_images, K=1)\n",
        "    augmented_unlabeled_X = augment(train_unlabeled_images, K=K)\n",
        "    \n",
        "    print(\"shape after\", augmented_unlabeled_X.shape )\n",
        "    \n",
        "    # Take a look at some of the augmented images\n",
        "    # displayImages(train_labeled_images[:10], title1=\"Original-Labeled\", title2=\"Augmented-Labeled\",\n",
        "    #               augmented_images=augmented_labeled_X[:10], labels=train_labeled_targets[:10])\n",
        "    # n_unlabeled = train_unlabeled_images.shape[0]\n",
        "    # displayImages(train_unlabeled_images[:10], title1=\"Original-Unlabeled\", title2=\"Augmented-Unlabeled\",\n",
        "    #               augmented_images=augmented_unlabeled_X[:10], labels=train_unlabeled_targets[:10])\n",
        "    # displayImages(augmented_unlabeled_X[:10], title1=\"Augmented-Unlabeled1\", title2=\"Augmented-Unlabeled2\",\n",
        "    #               augmented_images=augmented_unlabeled_X[n_unlabeled:10+n_unlabeled],\n",
        "    #               labels=train_unlabeled_targets[:10])\n",
        "\n",
        "    # Step 7: Change the dimension of np.array in oder for it to work in torch\n",
        "    augmented_labeled_X = to_tensor_dim(augmented_labeled_X.reshape(train_labeled_images.shape))\n",
        "\n",
        "    augmented_unlabeled_X_zeros = np.zeros(([K]+[augmented_unlabeled_X.shape[1]] + list(augmented_labeled_X.shape[1:])))\n",
        "    #print(\"augmented_unlabeled_X_zeros\", augmented_unlabeled_X_zeros.shape)\n",
        "    for k in range(K):\n",
        "      unlabeled_shape = augmented_unlabeled_X[k].reshape(train_unlabeled_images.shape)\n",
        "      #print(\"unlabeled_shape\", unlabeled_shape)\n",
        "      augmented_unlabeled_X_zeros[k] = to_tensor_dim(unlabeled_shape)\n",
        "    validation_images = to_tensor_dim(validation_images)\n",
        "    \n",
        "    return torch.from_numpy(augmented_labeled_X), torch.from_numpy(augmented_unlabeled_X_zeros), \\\n",
        "           torch.from_numpy(train_labeled_targets), torch.from_numpy(train_unlabeled_targets), \\\n",
        "           torch.from_numpy(validation_images), torch.from_numpy(validation_targets)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "f40670dac4654f839173b68e5e1f195b",
            "f287381ea5b842c1b44f01f4c5c570d2",
            "4c11d853c1a94d80ba3ed156a61c87a8",
            "c7624da01ae2498ea56ac0b65538ca9c",
            "42076a36481b4fe0a43f680973e9d781",
            "504977c5b3c14b53befee17d71632dac",
            "54d4be4b082e465ea2976f721ee68399",
            "a9a71610cab642b39167092c366dbb15"
          ]
        },
        "id": "4t_9ML18yasD",
        "outputId": "846677f1-a4a3-437c-9fd7-493ae4acf590"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    \"n_classes\": 10,\n",
        "    \"n_labeled_per_class\": 3000,\n",
        "    \"n_validation\": 500,\n",
        "    \"K\": 2\n",
        "}\n",
        "augmented_labeled_X, augmented_unlabeled_X, train_labeled_targets, train_unlabeled_targets, \\\n",
        "    validation_images, validation_targets = load_and_augment_data('CIFAR10', model_params)\n",
        "\n",
        "print(augmented_labeled_X.size())\n",
        "print(augmented_unlabeled_X.size())\n",
        "print(train_labeled_targets.size())\n",
        "print(train_unlabeled_targets.size())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f40670dac4654f839173b68e5e1f195b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./datasets/cifar-10-python.tar.gz to ./datasets\n",
            "shape (15000, 32, 32, 3)\n",
            "shape after (2, 15000, 32, 32, 3)\n",
            "torch.Size([30000, 3, 32, 32])\n",
            "torch.Size([2, 15000, 3, 32, 32])\n",
            "torch.Size([30000])\n",
            "torch.Size([15000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttYoAOvqk1ot"
      },
      "source": [
        "permutation = torch.randperm(augmented_labeled_X.size()[1])\n",
        "\n",
        "batches = []\n",
        "for i in range(0,augmented_labeled_X.size()[1], 10):\n",
        "        indices = permutation[i:i+10]\n",
        "        batch_x, batch_y = augmented_labeled_X[indices], train_labeled_targets[indices]\n",
        "        batches.append((batch_x, batch_y))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5_MqEolyPr",
        "outputId": "91eafb96-2c74-4511-e956-77ec23aac97e"
      },
      "source": [
        "batches"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([[[[ 3.1227e-01,  5.5036e-01, -2.9092e-01,  ..., -7.1949e-01,\n",
              "             -7.3537e-01, -7.5124e-01],\n",
              "            [ 4.8687e-01,  1.0107e+00,  9.0041e-02,  ..., -7.0362e-01,\n",
              "             -7.3537e-01, -7.5124e-01],\n",
              "            [ 3.1227e-01,  5.5036e-01, -2.9092e-01,  ..., -7.1949e-01,\n",
              "             -7.3537e-01, -7.5124e-01],\n",
              "            ...,\n",
              "            [ 1.2488e+00,  1.2012e+00,  1.1377e+00,  ...,  1.0107e+00,\n",
              "              1.0107e+00,  7.4084e-01],\n",
              "            [ 1.2329e+00,  1.2170e+00,  1.0742e+00,  ...,  1.4551e+00,\n",
              "              1.3916e+00,  7.4084e-01],\n",
              "            [ 1.1535e+00,  1.1218e+00,  1.0742e+00,  ...,  1.2329e+00,\n",
              "              1.1853e+00,  7.7259e-01]],\n",
              "  \n",
              "           [[ 4.8629e-02, -9.6262e-02, -8.8512e-01,  ..., -7.5632e-01,\n",
              "             -7.7242e-01, -7.8852e-01],\n",
              "            [ 4.6720e-01,  7.5699e-01, -4.7965e-02,  ..., -7.2413e-01,\n",
              "             -7.7242e-01, -7.8852e-01],\n",
              "            [ 4.8629e-02, -9.6262e-02, -8.8512e-01,  ..., -7.5632e-01,\n",
              "             -7.7242e-01, -7.8852e-01],\n",
              "            ...,\n",
              "            [ 7.2479e-01,  7.0869e-01,  6.6039e-01,  ...,  6.7649e-01,\n",
              "              6.2820e-01,  3.0621e-01],\n",
              "            [ 6.6039e-01,  7.2479e-01,  6.4429e-01,  ...,  1.2239e+00,\n",
              "              1.0951e+00,  3.0621e-01],\n",
              "            [ 6.2820e-01,  6.4429e-01,  6.1210e-01,  ...,  8.8578e-01,\n",
              "              8.3748e-01,  3.5451e-01]],\n",
              "  \n",
              "           [[-8.7390e-02, -1.9234e-01, -1.0169e+00,  ..., -5.5216e-01,\n",
              "             -5.6716e-01, -5.8215e-01],\n",
              "            [ 4.6734e-01,  7.8219e-01, -8.7390e-02,  ..., -5.2218e-01,\n",
              "             -5.6716e-01, -5.8215e-01],\n",
              "            [-8.7390e-02, -1.9234e-01, -1.0169e+00,  ..., -5.5216e-01,\n",
              "             -5.6716e-01, -5.8215e-01],\n",
              "            ...,\n",
              "            [ 3.6239e-01,  3.0242e-01,  2.4245e-01,  ...,  2.2746e-01,\n",
              "              1.8248e-01, -4.2412e-02],\n",
              "            [ 2.5744e-01,  3.0242e-01,  2.2746e-01,  ...,  7.0722e-01,\n",
              "              5.2731e-01, -7.2398e-02],\n",
              "            [ 1.9747e-01,  2.1246e-01,  1.9747e-01,  ...,  4.2236e-01,\n",
              "              3.4740e-01, -1.2427e-02]]],\n",
              "  \n",
              "  \n",
              "          [[[-1.9568e-01, -3.0679e-01, -3.0679e-01,  ..., -6.5600e-01,\n",
              "             -5.7663e-01, -6.2425e-01],\n",
              "            [-2.1155e-01, -2.4330e-01, -2.2742e-01,  ..., -4.1790e-01,\n",
              "             -2.5917e-01, -3.7028e-01],\n",
              "            [-1.4806e-01, -2.5917e-01, -1.4806e-01,  ..., -3.8616e-01,\n",
              "             -3.3854e-01, -2.9092e-01],\n",
              "            ...,\n",
              "            [-9.2585e-01, -7.5124e-01,  3.1227e-01,  ..., -6.5600e-01,\n",
              "             -8.7823e-01, -5.2901e-01],\n",
              "            [-8.9410e-01, -9.2585e-01,  2.1703e-01,  ..., -2.7504e-01,\n",
              "             -2.7504e-01, -2.1155e-01],\n",
              "            [-1.6393e-01, -1.7980e-01,  8.5195e-01,  ...,  4.7100e-01,\n",
              "              4.5512e-01,  3.9163e-01]],\n",
              "  \n",
              "           [[-3.0555e-01, -3.6995e-01, -3.3775e-01,  ..., -7.7242e-01,\n",
              "             -7.2413e-01, -8.2072e-01],\n",
              "            [-3.6995e-01, -4.1824e-01, -3.5385e-01,  ..., -5.6313e-01,\n",
              "             -4.5044e-01, -6.1143e-01],\n",
              "            [-3.8605e-01, -5.4704e-01, -4.0214e-01,  ..., -5.6313e-01,\n",
              "             -5.7923e-01, -5.6313e-01],\n",
              "            ...,\n",
              "            [-9.9781e-01, -8.0462e-01,  3.0621e-01,  ..., -1.2232e+00,\n",
              "             -1.3842e+00, -1.0783e+00],\n",
              "            [-9.6561e-01, -9.8171e-01,  2.0962e-01,  ..., -8.8512e-01,\n",
              "             -8.5292e-01, -8.0462e-01],\n",
              "            [-2.0896e-01, -2.0896e-01,  8.6968e-01,  ..., -8.0163e-02,\n",
              "             -6.4064e-02, -1.2846e-01]],\n",
              "  \n",
              "           [[-3.2727e-01, -4.4722e-01, -3.7225e-01,  ..., -7.4707e-01,\n",
              "             -7.3208e-01, -8.6701e-01],\n",
              "            [-4.4722e-01, -5.6716e-01, -4.6221e-01,  ..., -5.8215e-01,\n",
              "             -5.0719e-01, -6.8710e-01],\n",
              "            [-5.2218e-01, -7.7705e-01, -5.8215e-01,  ..., -6.1213e-01,\n",
              "             -6.5711e-01, -6.8710e-01],\n",
              "            ...,\n",
              "            [-1.1219e+00, -9.1199e-01,  2.2746e-01,  ..., -1.4367e+00,\n",
              "             -1.5567e+00, -1.3468e+00],\n",
              "            [-1.0919e+00, -1.0919e+00,  1.2251e-01,  ..., -1.1669e+00,\n",
              "             -1.1219e+00, -1.1069e+00],\n",
              "            [-3.4227e-01, -3.1228e-01,  7.8219e-01,  ..., -3.1228e-01,\n",
              "             -2.9729e-01, -3.7225e-01]]],\n",
              "  \n",
              "  \n",
              "          [[[-3.3854e-01, -3.8616e-01, -3.2266e-01,  ..., -1.6393e-01,\n",
              "             -5.2818e-02, -1.6393e-01],\n",
              "            [-3.3854e-01, -4.0203e-01, -3.7028e-01,  ..., -4.0203e-01,\n",
              "             -2.4330e-01, -4.0203e-01],\n",
              "            [-3.6945e-02, -1.0044e-01,  2.6548e-02,  ..., -7.6711e-01,\n",
              "             -7.0362e-01, -7.6711e-01],\n",
              "            ...,\n",
              "            [-1.7671e+00, -1.8465e+00, -1.7830e+00,  ...,  1.2647e+00,\n",
              "              1.2329e+00,  1.2647e+00],\n",
              "            [-1.4973e+00, -1.7830e+00, -1.5925e+00,  ...,  1.3123e+00,\n",
              "              1.2805e+00,  1.3123e+00],\n",
              "            [-9.5759e-01, -1.2274e+00, -9.7346e-01,  ...,  8.2021e-01,\n",
              "              8.3608e-01,  8.2021e-01]],\n",
              "  \n",
              "           [[-2.0896e-01, -2.0896e-01, -1.2846e-01,  ...,  1.9352e-01,\n",
              "              3.0621e-01,  1.9352e-01],\n",
              "            [-2.7335e-01, -2.7335e-01, -2.0896e-01,  ..., -1.6066e-01,\n",
              "              3.3212e-04, -1.6066e-01],\n",
              "            [-8.0163e-02, -1.4456e-01, -1.5767e-02,  ..., -5.3094e-01,\n",
              "             -4.5044e-01, -5.3094e-01],\n",
              "            ...,\n",
              "            [-1.8833e+00, -1.8672e+00, -1.7867e+00,  ...,  6.4728e-02,\n",
              "              3.2530e-02,  6.4728e-02],\n",
              "            [-1.7062e+00, -1.8672e+00, -1.6901e+00,  ...,  1.1303e-01,\n",
              "              4.8629e-02,  1.1303e-01],\n",
              "            [-1.1427e+00, -1.2876e+00, -1.0622e+00,  ...,  1.6431e-02,\n",
              "              4.8629e-02,  1.6431e-02]],\n",
              "  \n",
              "           [[-6.1213e-01, -5.9714e-01, -4.3222e-01,  ...,  1.3750e-01,\n",
              "              2.4245e-01,  1.3750e-01],\n",
              "            [-6.8710e-01, -6.2713e-01, -4.7720e-01,  ..., -2.3732e-01,\n",
              "             -1.1738e-01, -2.3732e-01],\n",
              "            [-4.6221e-01, -4.4722e-01, -2.5231e-01,  ..., -6.5711e-01,\n",
              "             -6.2713e-01, -6.5711e-01],\n",
              "            ...,\n",
              "            [-1.6616e+00, -1.6017e+00, -1.5417e+00,  ..., -5.5216e-01,\n",
              "             -6.1213e-01, -5.5216e-01],\n",
              "            [-1.5417e+00, -1.6017e+00, -1.4517e+00,  ..., -3.7225e-01,\n",
              "             -4.1723e-01, -3.7225e-01],\n",
              "            [-1.0919e+00, -1.1519e+00, -9.4197e-01,  ..., -4.7720e-01,\n",
              "             -4.1723e-01, -4.7720e-01]]]], dtype=torch.float64),\n",
              "  tensor([7, 4, 4]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY3wObVfFWXW",
        "outputId": "adf773e7-eec3-4a9a-935c-fd3912624b8a"
      },
      "source": [
        "\n",
        "a = torch.arange(10).reshape(5,2)\n",
        "b = torch.split(a, [1,4])\n",
        "type(b[0])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHjp1npZFjyz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPSoU3xYzGPD"
      },
      "source": [
        "import torch.utils.data as data\n",
        "batch_size = 10\n",
        "labeled_trainloader = load((augmented_labeled_X, train_labeled_targets), batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqBkDdjS-c9Y"
      },
      "source": [
        "labeled_trainloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q20cZv5yC8H8"
      },
      "source": [
        "labeled_train_iter = iter(labeled_trainloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HajD3KshC98p"
      },
      "source": [
        "inputs_x, targets_x = labeled_train_iter.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPSME6c5yzb-"
      },
      "source": [
        "def mix_match(x_batch, y_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZiPAa-7zFTu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fma7aqeC9ioS"
      },
      "source": [
        "def train_val_split(labels, n_labeled_per_class):\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_idxs = []\n",
        "    train_unlabeled_idxs = []\n",
        "    val_idxs = []\n",
        "\n",
        "    for i in range(10):\n",
        "        idxs = np.where(labels == i)[0]\n",
        "        np.random.shuffle(idxs)\n",
        "        train_labeled_idxs.extend(idxs[:n_labeled_per_class])\n",
        "        train_unlabeled_idxs.extend(idxs[n_labeled_per_class:-500])\n",
        "        val_idxs.extend(idxs[-500:])\n",
        "    np.random.shuffle(train_labeled_idxs)\n",
        "    np.random.shuffle(train_unlabeled_idxs)\n",
        "    np.random.shuffle(val_idxs)\n",
        "\n",
        "    return train_labeled_idxs, train_unlabeled_idxs, val_idxs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q6ebyri97fk"
      },
      "source": [
        "class CIFAR10_labeled(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(self, root, indexs=None, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(CIFAR10_labeled, self).__init__(root, train=train,\n",
        "                 transform=transform, target_transform=target_transform,\n",
        "                 download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "        self.data = transpose(normalise(self.data))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "    \n",
        "\n",
        "class CIFAR10_unlabeled(CIFAR10_labeled):\n",
        "\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(CIFAR10_unlabeled, self).__init__(root, indexs, train=train,\n",
        "                 transform=transform, target_transform=target_transform,\n",
        "                 download=download)\n",
        "        self.targets = np.array([-1 for i in range(len(self.targets))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "dG_EAv26-kRT",
        "outputId": "71292b88-4b79-4f46-c336-1c92690f97bf"
      },
      "source": [
        "def normalise(x, mean=cifar10_mean, std=cifar10_std):\n",
        "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
        "    x -= mean*255\n",
        "    x *= 1.0/(255*std)\n",
        "    return x\n",
        "\n",
        "def transpose(x, source='NHWC', target='NCHW'):\n",
        "    return x.transpose([source.index(d) for d in target]) \n",
        "\n",
        "def pad(x, border=4):\n",
        "    return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')\n",
        "\n",
        "class RandomPadandCrop(object):\n",
        "    \"\"\"Crop randomly the image.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = pad(x, 4)\n",
        "\n",
        "        h, w = x.shape[1:]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = np.random.randint(0, h - new_h)\n",
        "        left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        x = x[:, top: top + new_h, left: left + new_w]\n",
        "\n",
        "        return x\n",
        "\n",
        "class RandomFlip(object):\n",
        "    \"\"\"Flip randomly the image.\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        if np.random.rand() < 0.5:\n",
        "            x = x[:, :, ::-1]\n",
        "\n",
        "        return x.copy()\n",
        "\n",
        "class GaussianNoise(object):\n",
        "    \"\"\"Add gaussian noise to the image.\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        c, h, w = x.shape\n",
        "        x += np.random.randn(c, h, w) * 0.15\n",
        "        return x\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Transform the image to tensor.\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        x = torch.from_numpy(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ba822542371d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar10_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar10_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cifar10_mean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a19u2L65mzg"
      },
      "source": [
        "def get_cifar10(root, n_labeled,\n",
        "                 transform_train=None, transform_val=None,\n",
        "                 download=True):\n",
        "\n",
        "    base_dataset = torchvision.datasets.CIFAR10(root, train=True, download=download)\n",
        "    train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(base_dataset.targets, int(n_labeled/10))\n",
        "\n",
        "    train_labeled_dataset = CIFAR10_labeled(root, train_labeled_idxs, train=True, transform=transform_train)\n",
        "    train_unlabeled_dataset = CIFAR10_unlabeled(root, train_unlabeled_idxs, train=True, transform=TransformTwice(transform_train))\n",
        "    val_dataset = CIFAR10_labeled(root, val_idxs, train=True, transform=transform_val, download=True)\n",
        "    test_dataset = CIFAR10_labeled(root, train=False, transform=transform_val, download=True)\n",
        "\n",
        "    print (f\"#Labeled: {len(train_labeled_idxs)} #Unlabeled: {len(train_unlabeled_idxs)} #Val: {len(val_idxs)}\")\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Rf8CR66I3-",
        "outputId": "15484551-778c-420b-fa88-885355306eb0"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVr5iu0e_85l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR6RurC76TkG",
        "outputId": "577b61e9-52fb-45c0-d334-2fca83de429e"
      },
      "source": [
        "%mkdir DLDS_Data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜DLDS_Dataâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwOu3qE_i3b",
        "outputId": "e1de0313-a4c3-45e6-b5dd-3726dd33b910"
      },
      "source": [
        "X = datasets.CIFAR10(root='/content/gdrive/MyDrive/DLDS_Data', train=True, download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsShVQF86ZmV"
      },
      "source": [
        "# n_labeled = 100\n",
        "# train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset = get_cifar10(\"/DLDS_Data\", n_labeled)\n",
        "\n",
        "data_dir = '/content/gdrive/MyDrive/DLDS_Data'\n",
        "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "#                                           data_transforms[x])\n",
        "#                   for x in ['train', 'val']}\n",
        "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "#                                              shuffle=True, num_workers=4)\n",
        "#               for x in ['train', 'val']}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhpTM2QdBQTu",
        "outputId": "e631549f-ea13-4ab2-9fbb-4310b1397e56"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=data_dir , train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=data_dir , train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "dataloaders = {\"train\": trainloader}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gHRMa3UJytW",
        "outputId": "c14ca953-2a36-4fca-b7d3-0ac76617a10b"
      },
      "source": [
        "trainloader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f3310ef2190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq95SB7GDE6z"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                #import pdb; pdb.set_trace()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    #print(outputs.shape)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / 50000\n",
        "            epoch_acc = running_corrects.double() / 50000\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq7eEuUF8U-U"
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(classes))\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "nuS7KiCv8bEE",
        "outputId": "b81033d3-273d-45d5-f7b5-d37588281c71"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-cc88ea5f8bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=25)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-429fe8bbe977>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_sizes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DmwPhC-M8cQ4",
        "outputId": "d26da66a-9843-4a04-bcbc-c28bbfb69c28"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tBeB-FyHidw",
        "outputId": "d929cbcc-393d-4d67-a664-d1dc41f0c95c"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IQL5M-XHnl9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}