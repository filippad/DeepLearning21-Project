{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " MixMatch_v3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b9e3e27d7f1845e3b04f2de0d9d3b96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c41c36bc2f544969bf019ea0f7eba45",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4aaf5e47b00242e5ac2630cf182b8eef",
              "IPY_MODEL_6f6a3e64bd2942cca46f987a73c0da07"
            ]
          }
        },
        "0c41c36bc2f544969bf019ea0f7eba45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4aaf5e47b00242e5ac2630cf182b8eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b2c0c1d7d1714f1a95e6dca70fa68c46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2071557e6d6b46e68d3d01180ac80618"
          }
        },
        "6f6a3e64bd2942cca46f987a73c0da07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bad53864474f498480490bd354f11b57",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [08:02&lt;00:00, 353319.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8fb05f1a7d848998d13a3f006a03957"
          }
        },
        "b2c0c1d7d1714f1a95e6dca70fa68c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2071557e6d6b46e68d3d01180ac80618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bad53864474f498480490bd354f11b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8fb05f1a7d848998d13a3f006a03957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HdPUmwf4M-4",
        "outputId": "9f4f282c-e07f-47f6-d9dd-9f06a3fcb1ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AziIsuVg4X4W"
      },
      "source": [
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va2Mxic-yRlU"
      },
      "source": [
        "def displayImages(images, title1=\"Original\", title2=\"Augmented\", labels=None, augmented_images=None):\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    fig = plt.figure(figsize=(images.shape[0], images.shape[0]))\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        if labels is not None:\n",
        "            plt.xlabel(class_names[int(labels[i])])\n",
        "    fig.suptitle(title1, fontsize=16)\n",
        "\n",
        "    if augmented_images is not None:\n",
        "        fig2 = plt.figure(2, figsize=(augmented_images.shape[0], augmented_images.shape[0]))\n",
        "        for i in range(augmented_images.shape[0]):\n",
        "            plt.subplot(5, 5, i + 1)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.grid(False)\n",
        "            plt.imshow(augmented_images[i], cmap=plt.cm.binary)\n",
        "            if labels is not None:\n",
        "                plt.xlabel(class_names[int(labels[i])])\n",
        "        fig2.suptitle(title2, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def split_indexes(n_classes, n_labeled_per_class, n_validation, labels):\n",
        "    labels = np.array(labels)\n",
        "    train_labeled_indexes = []\n",
        "    train_unlabeled_indexes = []\n",
        "    validation_indexes = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        indexes = np.where(labels == i)[0]\n",
        "        np.random.shuffle(indexes)\n",
        "\n",
        "        train_labeled_indexes.extend(indexes[:n_labeled_per_class])\n",
        "        train_unlabeled_indexes.extend(indexes[n_labeled_per_class:-n_validation])\n",
        "        validation_indexes.extend(indexes[-n_validation:])\n",
        "\n",
        "    np.random.shuffle(train_unlabeled_indexes)\n",
        "    np.random.shuffle(train_labeled_indexes)\n",
        "    np.random.shuffle(validation_indexes)\n",
        "\n",
        "    return train_labeled_indexes, train_unlabeled_indexes, validation_indexes\n",
        "\n",
        "\n",
        "def to_tensor_dim(x, source='NHWC', target='NCHW'):\n",
        "    return x.transpose([source.index(d) for d in target])\n",
        "\n",
        "\n",
        "def normalise(X):\n",
        "    mean = np.mean(X, axis=(0, 1, 2))\n",
        "    std = np.std(X, axis=(0, 1, 2))\n",
        "    X, mean, std = [np.array(a, np.float32) for a in (X, mean, std)]\n",
        "    X -= mean\n",
        "    X *= 1.0 / std\n",
        "    return X\n",
        "\n",
        "\n",
        "def normalise2(X):\n",
        "    x = X.copy()\n",
        "    mean = np.mean(x, axis=(0, 1, 2)) / 255\n",
        "    std = np.std(x, axis=(0, 1, 2)) / 255\n",
        "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
        "    x -= mean * 255\n",
        "    x *= 1.0 / (255 * std)\n",
        "    return x\n",
        "\n",
        "\n",
        "def random_flip(x):\n",
        "    if np.random.rand() < 0.6:\n",
        "        x = x[:, ::-1, :]\n",
        "    return x.copy()\n",
        "\n",
        "\n",
        "def pad(x, border=4):\n",
        "    return np.pad(x, [(border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "\n",
        "def pad_and_crop(x, output_size=(32, 32)):\n",
        "    x = pad(x, 4)\n",
        "    h, w = x.shape[:-1]\n",
        "    new_h, new_w = output_size\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    x = x[top: top + new_h, left: left + new_w, :]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def augment(X, K=1):\n",
        "\n",
        "    # X_augmented = X\n",
        "    # for k in range(K-1):\n",
        "    #   X_augmented.hstack(X)\n",
        "    X_augmented = np.zeros(([K]+list(X.shape)))\n",
        "\n",
        "    # print(\"after hstack\", X_augmented.shape)\n",
        "    for k in range(K):\n",
        "      for i in range(X_augmented[k].shape[0]):\n",
        "            x = X[i, :]\n",
        "            x = pad_and_crop(x)\n",
        "            X_augmented[k, i, :] = random_flip(x)\n",
        "    return X_augmented\n",
        "\n",
        "\n",
        "def load_and_augment_data(dataset_name, model_params):\n",
        "    \"\"\"\n",
        "    From datasets.CIFAR10:\n",
        "        dataset.data: the image as numpy array, shape: (50000, 32, 32, 3)\n",
        "        dataset.targets: labels of the images as list, len: 50000\n",
        "    :return:\n",
        "        augmented_labeled_X: the tensor of augmented labeled images (K=1),\n",
        "                             size: (n_labeled_per_class * n_classes , 32, 32, 3)\n",
        "        augmented_unlabeled_X: the tensor of augmented unlabeled images (K=2),\n",
        "                             size: ((N/10 - n_labeled_per_class - n_validation) * n_classes * K , 32, 32, 3)\n",
        "        train_labeled_targets: the tensor of labeled targets,\n",
        "                             size = n_labeled_per_class * n_classes\n",
        "        train_unlabeled_targets: the tensor of unlabeled targets,\n",
        "                             size = (N/10 - n_labeled_per_class - n_validation) * n_classes\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Set the model's hyperparameters\n",
        "    n_classes = model_params[\"n_classes\"]\n",
        "    n_labeled_per_class = model_params[\"n_labeled_per_class\"]\n",
        "    n_validation = model_params[\"n_validation\"]\n",
        "    K = model_params[\"K\"]\n",
        "\n",
        "    # Step 2: Load the dataset\n",
        "    if dataset_name == 'CIFAR10':\n",
        "        dataset = datasets.CIFAR10(root=\"./datasets\", train=True, download=True)\n",
        "    elif dataset_name == 'SLT10':\n",
        "        dataset = datasets.STL10(root=\"./datasets\", download=True)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset name\")\n",
        "\n",
        "    # Step 3: Split the indexes\n",
        "    train_labeled_indexes, train_unlabeled_indexes, validation_indexes = \\\n",
        "        split_indexes(n_classes, n_labeled_per_class, n_validation, dataset.targets)\n",
        "\n",
        "    # Step 4: Attract the images for training, validation\n",
        "    train_labeled_images = np.take(dataset.data, train_labeled_indexes, axis=0)\n",
        "    train_unlabeled_images = np.take(dataset.data, train_unlabeled_indexes, axis=0)\n",
        "    target_array = np.asarray(dataset.targets)\n",
        "    train_labeled_targets = np.take(target_array, train_labeled_indexes, axis=0)\n",
        "    train_unlabeled_targets = np.take(target_array, train_unlabeled_indexes, axis=0)\n",
        "    validation_images = np.take(dataset.data, validation_indexes, axis=0)\n",
        "    validation_targets = np.take(target_array, validation_indexes, axis=0)\n",
        "\n",
        "    # Step 5: Normalise the datasets\n",
        "    train_labeled_images = normalise(train_labeled_images)\n",
        "    train_unlabeled_images = normalise(train_unlabeled_images)\n",
        "\n",
        "    # Step 6: Augment training images\n",
        "    print(\"shape\",train_unlabeled_images.shape )\n",
        "\n",
        "    augmented_labeled_X = augment(train_labeled_images, K=1)\n",
        "    augmented_unlabeled_X = augment(train_unlabeled_images, K=K)\n",
        "    \n",
        "    print(\"shape after\", augmented_unlabeled_X.shape )\n",
        "    \n",
        "    # Take a look at some of the augmented images\n",
        "    # displayImages(train_labeled_images[:10], title1=\"Original-Labeled\", title2=\"Augmented-Labeled\",\n",
        "    #               augmented_images=augmented_labeled_X[:10], labels=train_labeled_targets[:10])\n",
        "    # n_unlabeled = train_unlabeled_images.shape[0]\n",
        "    # displayImages(train_unlabeled_images[:10], title1=\"Original-Unlabeled\", title2=\"Augmented-Unlabeled\",\n",
        "    #               augmented_images=augmented_unlabeled_X[:10], labels=train_unlabeled_targets[:10])\n",
        "    # displayImages(augmented_unlabeled_X[:10], title1=\"Augmented-Unlabeled1\", title2=\"Augmented-Unlabeled2\",\n",
        "    #               augmented_images=augmented_unlabeled_X[n_unlabeled:10+n_unlabeled],\n",
        "    #               labels=train_unlabeled_targets[:10])\n",
        "\n",
        "    # Step 7: Change the dimension of np.array in oder for it to work in torch\n",
        "    augmented_labeled_X = to_tensor_dim(augmented_labeled_X.reshape(train_labeled_images.shape))\n",
        "\n",
        "    augmented_unlabeled_X_zeros = np.zeros(([K]+[augmented_unlabeled_X.shape[1]] + list(augmented_labeled_X.shape[1:])))\n",
        "    #print(\"augmented_unlabeled_X_zeros\", augmented_unlabeled_X_zeros.shape)\n",
        "    for k in range(K):\n",
        "      unlabeled_shape = augmented_unlabeled_X[k].reshape(train_unlabeled_images.shape)\n",
        "      #print(\"unlabeled_shape\", unlabeled_shape)\n",
        "      augmented_unlabeled_X_zeros[k] = to_tensor_dim(unlabeled_shape)\n",
        "    validation_images = to_tensor_dim(validation_images)\n",
        "    \n",
        "    return torch.from_numpy(augmented_labeled_X), torch.from_numpy(augmented_unlabeled_X_zeros), \\\n",
        "           torch.from_numpy(train_labeled_targets), torch.from_numpy(train_unlabeled_targets), \\\n",
        "           torch.from_numpy(validation_images), torch.from_numpy(validation_targets)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVktFDszObih"
      },
      "source": [
        "# Defining Model Parameters and obtaining train and val sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "b9e3e27d7f1845e3b04f2de0d9d3b96e",
            "0c41c36bc2f544969bf019ea0f7eba45",
            "4aaf5e47b00242e5ac2630cf182b8eef",
            "6f6a3e64bd2942cca46f987a73c0da07",
            "b2c0c1d7d1714f1a95e6dca70fa68c46",
            "2071557e6d6b46e68d3d01180ac80618",
            "bad53864474f498480490bd354f11b57",
            "c8fb05f1a7d848998d13a3f006a03957"
          ]
        },
        "id": "4t_9ML18yasD",
        "outputId": "bb9c8f4b-f0f5-4d5f-c3a3-2d04e08f335a"
      },
      "source": [
        "model_params = {\n",
        "    \"n_classes\": 10,\n",
        "    \"n_labeled_per_class\": 3000,\n",
        "    \"n_validation\": 500,\n",
        "    \"K\": 2\n",
        "}\n",
        "augmented_labeled_X, augmented_unlabeled_X, train_labeled_targets, train_unlabeled_targets, \\\n",
        "    validation_images, validation_targets = load_and_augment_data('CIFAR10', model_params)\n",
        "\n",
        "print(augmented_labeled_X.size())\n",
        "print(augmented_unlabeled_X.size())\n",
        "print(train_labeled_targets.size())\n",
        "print(train_unlabeled_targets.size())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9e3e27d7f1845e3b04f2de0d9d3b96e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./datasets/cifar-10-python.tar.gz to ./datasets\n",
            "shape (15000, 32, 32, 3)\n",
            "shape after (2, 15000, 32, 32, 3)\n",
            "torch.Size([30000, 3, 32, 32])\n",
            "torch.Size([2, 15000, 3, 32, 32])\n",
            "torch.Size([30000])\n",
            "torch.Size([15000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbp2lWcuJYmV"
      },
      "source": [
        "# Splitting data into Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pok_bZgUJxco"
      },
      "source": [
        "batch_size = 10"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6c1XbUJkA2"
      },
      "source": [
        "## Splitting labelled data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttYoAOvqk1ot"
      },
      "source": [
        "permutation = torch.randperm(augmented_labeled_X.size()[0])\n",
        "labeled_batches = []\n",
        "for i in range(0,augmented_labeled_X.size()[1], batch_size):\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_x, batch_y = augmented_labeled_X[indices], train_labeled_targets[indices]\n",
        "        labeled_batches.append((batch_x, batch_y))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn5yUae9Jua8"
      },
      "source": [
        "# Splitting unlabelled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMO1gm-wItV9"
      },
      "source": [
        "#Splitting unlabelled into batches\n",
        "k_unlabeled_batches = []\n",
        "for k in range(augmented_unlabeled_X.size()[0]):\n",
        "  permutation = torch.randperm(augmented_unlabeled_X.size()[1])\n",
        "  unlabeled_batches = []\n",
        "  for i in range(0, augmented_unlabeled_X.size()[1], batch_size):\n",
        "          indices = permutation[i:i+batch_size]\n",
        "          batch = augmented_unlabeled_X[k][indices]\n",
        "          unlabeled_batches.append(batch)\n",
        "  k_unlabeled_batches.append(unlabeled_batches)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5_MqEolyPr",
        "outputId": "ba4d53ef-51c2-4dbe-dac7-cbb6c0b142c2"
      },
      "source": [
        "k_unlabeled_batches[0][0].size()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vhi_NtWR3UQ"
      },
      "source": [
        "K_unlabeled_batches is a tuple of shape [k, num_batches, batch_size, 3, 32, 32]\n",
        "where k is the number of augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okMxCrpwOTfa"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY3wObVfFWXW"
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(classes))\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHjp1npZFjyz"
      },
      "source": [
        " def label_guessing(model, data):\n",
        "   return model(data)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoVTPmq4SVon"
      },
      "source": [
        "# Example use of label_guessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDaNW_EYQiEe",
        "outputId": "d66032bb-154d-4124-cfe1-26f3ea6ec5a3"
      },
      "source": [
        "label_guessing(model_ft, k_unlabeled_batches[0][0].float().to(device))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7413,  0.2303, -1.1404, -0.9824,  0.4227, -0.0878, -0.9023, -0.1468,\n",
              "         -0.1301, -0.8961],\n",
              "        [ 0.8172, -0.3618, -0.6507, -0.6984,  0.0918, -0.3904, -0.4134, -0.2655,\n",
              "          0.9347,  1.5077],\n",
              "        [ 1.1301, -0.9810, -0.6539,  0.6671, -0.9301,  1.9188,  0.9120,  0.2618,\n",
              "          0.6883,  1.6490],\n",
              "        [ 0.1507,  0.6785,  0.0633, -0.3358,  0.6889, -0.2453, -0.9054, -0.8726,\n",
              "          0.2392,  0.8137],\n",
              "        [-0.2195, -0.1123,  0.4734, -0.0570, -0.4082, -0.3268, -0.9830, -1.0727,\n",
              "         -1.3056, -0.1220],\n",
              "        [ 0.6277, -0.4178, -0.5428, -0.0347,  0.8450,  0.3425, -0.5820, -0.1443,\n",
              "          1.1156,  1.1936],\n",
              "        [ 1.3575,  0.4967, -0.0581, -1.6985, -0.2460, -0.8160,  0.1886, -1.8309,\n",
              "          1.2961,  0.4708],\n",
              "        [-1.0465, -0.0949,  0.3711,  0.5996,  0.9751,  2.1919, -0.4540,  0.1942,\n",
              "          0.7604,  0.5180],\n",
              "        [ 1.7178,  0.9226,  1.4036,  2.0083, -0.6838,  1.4200,  0.6403,  1.2419,\n",
              "         -0.5672,  1.1809],\n",
              "        [-0.1213, -1.0192, -1.1332,  0.5552,  1.9711,  0.0956, -0.2000, -1.0844,\n",
              "          0.0872,  2.6767]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2KIoCSgRpD8"
      },
      "source": [
        "#Example Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8puyYVxCRn_q"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                #import pdb; pdb.set_trace()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    #print(outputs.shape)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / 50000\n",
        "            epoch_acc = running_corrects.double() / 50000\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8uTe8SKRxRb"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPSoU3xYzGPD"
      },
      "source": [
        "# import torch.utils.data as data\n",
        "# batch_size = 10\n",
        "# labeled_trainloader = load((augmented_labeled_X, train_labeled_targets), batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbxUeMgiRwrl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqBkDdjS-c9Y"
      },
      "source": [
        "# labeled_trainloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q20cZv5yC8H8"
      },
      "source": [
        "# labeled_train_iter = iter(labeled_trainloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HajD3KshC98p"
      },
      "source": [
        "# inputs_x, targets_x = labeled_train_iter.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPSME6c5yzb-"
      },
      "source": [
        "# def mix_match(x_batch, y_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZiPAa-7zFTu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fma7aqeC9ioS"
      },
      "source": [
        "# def train_val_split(labels, n_labeled_per_class):\n",
        "#     labels = np.array(labels)\n",
        "#     train_labeled_idxs = []\n",
        "#     train_unlabeled_idxs = []\n",
        "#     val_idxs = []\n",
        "\n",
        "#     for i in range(10):\n",
        "#         idxs = np.where(labels == i)[0]\n",
        "#         np.random.shuffle(idxs)\n",
        "#         train_labeled_idxs.extend(idxs[:n_labeled_per_class])\n",
        "#         train_unlabeled_idxs.extend(idxs[n_labeled_per_class:-500])\n",
        "#         val_idxs.extend(idxs[-500:])\n",
        "#     np.random.shuffle(train_labeled_idxs)\n",
        "#     np.random.shuffle(train_unlabeled_idxs)\n",
        "#     np.random.shuffle(val_idxs)\n",
        "\n",
        "#     return train_labeled_idxs, train_unlabeled_idxs, val_idxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q6ebyri97fk"
      },
      "source": [
        "# class CIFAR10_labeled(torchvision.datasets.CIFAR10):\n",
        "\n",
        "#     def __init__(self, root, indexs=None, train=True,\n",
        "#                  transform=None, target_transform=None,\n",
        "#                  download=False):\n",
        "#         super(CIFAR10_labeled, self).__init__(root, train=train,\n",
        "#                  transform=transform, target_transform=target_transform,\n",
        "#                  download=download)\n",
        "#         if indexs is not None:\n",
        "#             self.data = self.data[indexs]\n",
        "#             self.targets = np.array(self.targets)[indexs]\n",
        "#         self.data = transpose(normalise(self.data))\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             index (int): Index\n",
        "#         Returns:\n",
        "#             tuple: (image, target) where target is index of the target class.\n",
        "#         \"\"\"\n",
        "#         img, target = self.data[index], self.targets[index]\n",
        "\n",
        "#         if self.transform is not None:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         if self.target_transform is not None:\n",
        "#             target = self.target_transform(target)\n",
        "\n",
        "#         return img, target\n",
        "    \n",
        "\n",
        "# class CIFAR10_unlabeled(CIFAR10_labeled):\n",
        "\n",
        "#     def __init__(self, root, indexs, train=True,\n",
        "#                  transform=None, target_transform=None,\n",
        "#                  download=False):\n",
        "#         super(CIFAR10_unlabeled, self).__init__(root, indexs, train=train,\n",
        "#                  transform=transform, target_transform=target_transform,\n",
        "#                  download=download)\n",
        "#         self.targets = np.array([-1 for i in range(len(self.targets))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "dG_EAv26-kRT",
        "outputId": "71292b88-4b79-4f46-c336-1c92690f97bf"
      },
      "source": [
        "# def normalise(x, mean=cifar10_mean, std=cifar10_std):\n",
        "#     x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
        "#     x -= mean*255\n",
        "#     x *= 1.0/(255*std)\n",
        "#     return x\n",
        "\n",
        "# def transpose(x, source='NHWC', target='NCHW'):\n",
        "#     return x.transpose([source.index(d) for d in target]) \n",
        "\n",
        "# def pad(x, border=4):\n",
        "#     return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')\n",
        "\n",
        "# class RandomPadandCrop(object):\n",
        "#     \"\"\"Crop randomly the image.\n",
        "#     Args:\n",
        "#         output_size (tuple or int): Desired output size. If int, square crop\n",
        "#             is made.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, output_size):\n",
        "#         assert isinstance(output_size, (int, tuple))\n",
        "#         if isinstance(output_size, int):\n",
        "#             self.output_size = (output_size, output_size)\n",
        "#         else:\n",
        "#             assert len(output_size) == 2\n",
        "#             self.output_size = output_size\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         x = pad(x, 4)\n",
        "\n",
        "#         h, w = x.shape[1:]\n",
        "#         new_h, new_w = self.output_size\n",
        "\n",
        "#         top = np.random.randint(0, h - new_h)\n",
        "#         left = np.random.randint(0, w - new_w)\n",
        "\n",
        "#         x = x[:, top: top + new_h, left: left + new_w]\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class RandomFlip(object):\n",
        "#     \"\"\"Flip randomly the image.\n",
        "#     \"\"\"\n",
        "#     def __call__(self, x):\n",
        "#         if np.random.rand() < 0.5:\n",
        "#             x = x[:, :, ::-1]\n",
        "\n",
        "#         return x.copy()\n",
        "\n",
        "# class GaussianNoise(object):\n",
        "#     \"\"\"Add gaussian noise to the image.\n",
        "#     \"\"\"\n",
        "#     def __call__(self, x):\n",
        "#         c, h, w = x.shape\n",
        "#         x += np.random.randn(c, h, w) * 0.15\n",
        "#         return x\n",
        "\n",
        "# class ToTensor(object):\n",
        "#     \"\"\"Transform the image to tensor.\n",
        "#     \"\"\"\n",
        "#     def __call__(self, x):\n",
        "#         x = torch.from_numpy(x)\n",
        "#         return x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ba822542371d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar10_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcifar10_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cifar10_mean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a19u2L65mzg"
      },
      "source": [
        "# def get_cifar10(root, n_labeled,\n",
        "#                  transform_train=None, transform_val=None,\n",
        "#                  download=True):\n",
        "\n",
        "#     base_dataset = torchvision.datasets.CIFAR10(root, train=True, download=download)\n",
        "#     train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(base_dataset.targets, int(n_labeled/10))\n",
        "\n",
        "#     train_labeled_dataset = CIFAR10_labeled(root, train_labeled_idxs, train=True, transform=transform_train)\n",
        "#     train_unlabeled_dataset = CIFAR10_unlabeled(root, train_unlabeled_idxs, train=True, transform=TransformTwice(transform_train))\n",
        "#     val_dataset = CIFAR10_labeled(root, val_idxs, train=True, transform=transform_val, download=True)\n",
        "#     test_dataset = CIFAR10_labeled(root, train=False, transform=transform_val, download=True)\n",
        "\n",
        "#     print (f\"#Labeled: {len(train_labeled_idxs)} #Unlabeled: {len(train_unlabeled_idxs)} #Val: {len(val_idxs)}\")\n",
        "#     return train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Rf8CR66I3-",
        "outputId": "15484551-778c-420b-fa88-885355306eb0"
      },
      "source": [
        "# %cd /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVr5iu0e_85l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR6RurC76TkG",
        "outputId": "577b61e9-52fb-45c0-d334-2fca83de429e"
      },
      "source": [
        "# %mkdir DLDS_Data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜DLDS_Dataâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwOu3qE_i3b",
        "outputId": "e1de0313-a4c3-45e6-b5dd-3726dd33b910"
      },
      "source": [
        "# X = datasets.CIFAR10(root='/content/gdrive/MyDrive/DLDS_Data', train=True, download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsShVQF86ZmV"
      },
      "source": [
        "# # n_labeled = 100\n",
        "# # train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset = get_cifar10(\"/DLDS_Data\", n_labeled)\n",
        "\n",
        "# data_dir = '/content/gdrive/MyDrive/DLDS_Data'\n",
        "# # image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "# #                                           data_transforms[x])\n",
        "# #                   for x in ['train', 'val']}\n",
        "# # dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "# #                                              shuffle=True, num_workers=4)\n",
        "# #               for x in ['train', 'val']}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhpTM2QdBQTu",
        "outputId": "e631549f-ea13-4ab2-9fbb-4310b1397e56"
      },
      "source": [
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# batch_size = 4\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root=data_dir , train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=2)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(root=data_dir , train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# classes = ('plane', 'car', 'bird', 'cat',\n",
        "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# dataloaders = {\"train\": trainloader}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gHRMa3UJytW",
        "outputId": "c14ca953-2a36-4fca-b7d3-0ac76617a10b"
      },
      "source": [
        "# trainloader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f3310ef2190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq95SB7GDE6z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq7eEuUF8U-U"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "nuS7KiCv8bEE",
        "outputId": "b81033d3-273d-45d5-f7b5-d37588281c71"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-cc88ea5f8bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=25)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-429fe8bbe977>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_corrects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_sizes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DmwPhC-M8cQ4",
        "outputId": "d26da66a-9843-4a04-bcbc-c28bbfb69c28"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tBeB-FyHidw",
        "outputId": "f0c5d54c-1195-46e4-e8b8-d2fefaf82a81"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IQL5M-XHnl9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}